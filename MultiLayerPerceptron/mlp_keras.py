# -*- coding: utf-8 -*-
"""MLP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I-x4KZZkwEWqLuMY4Vsb_7Wke1zdTGYC
"""

import tensorflow as tf
import numpy as np
from tensorflow.keras.utils import to_categorical

# Load data 
mnist = tf.keras.datasets.mnist

# the data, split between train and validation sets
(x_train, y_train), (x_validation, y_validation) = mnist.load_data()

# convert images into one dimension from 28x28 pixels
x_train = x_train.reshape(60000, 784)
x_validation = x_validation.reshape(10000, 784)
x_train = x_train.astype('float32')
x_validation = x_validation.astype('float32')

# normalize into [0,1]
x_train /= 255
x_validation /= 255
print('train samples', x_train.shape)
print('test samples', x_validation.shape)

# if we want to use categorical_crossentropy loss instead of sparse we have to convert the "y" labels

y_train = to_categorical(y_train, num_classes=10)
y_validation = to_categorical(y_validation, num_classes=10)

print('train label samples', y_train.shape)
print('test label samples', y_validation.shape)

# Create a sequential model (neural network architecture)
model = tf.keras.Sequential()
# Building the architecture of our model by adding layers
model.add(tf.keras.layers.Dense(512, activation='relu', input_dim=784))
model.add(tf.keras.layers.BatchNormalization())

model.add(tf.keras.layers.Dense(256, activation='relu'))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(rate=0.3))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(rate=0.2))
model.add(tf.keras.layers.Dense(64, activation='relu'))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(rate=0.1))
model.add(tf.keras.layers.Dense(32, activation='relu'))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(rate=0.05))
model.add(tf.keras.layers.Dense(16, activation='relu'))
model.add(tf.keras.layers.BatchNormalization())
model.add(tf.keras.layers.Dropout(rate=0.01))
model.add(tf.keras.layers.Dense(10, activation='sigmoid'))

# parfois la normalisation ou le dropout peuvent affecté - l'accuracy

# Specifying how to update the weights → optimizers
adam = tf.keras.optimizers.Adam()

# or this optimizers
sgd = tf.keras.optimizers.SGD(learning_rate=0.02)
# meilleur que adam niveau accuracy

from tensorflow.keras.callbacks import ModelCheckpoint

checkpointer = ModelCheckpoint(filepath='model.hdf5', monitor='val_loss', verbose=1, save_best_only=True)

# Compile the model with adam
model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])

# Train model with fit for 10 epochs
history = model.fit(x_train, y_train, validation_data = (x_validation, y_validation), epochs=25, callbacks = [checkpointer], batch_size=16)
# plus le batch size est grand, moins il va update les poids donc moins de précision mais plus rapide 
# par defaut il est a 32

from matplotlib import pyplot as plt

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper right')
plt.show()

from tensorflow.keras.models import load_model
model = load_model('model.hdf5')

import cv2
from matplotlib import pyplot as plt

image = x_validation[19].reshape(28,28)
plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.axis('off')

datapoint = x_validation[19].reshape(1,784)
predict_prob = model.predict(datapoint)
print('I am confident around {:.2f}% that this image corresponds to digit {}'.format(np.amax(predict_prob)*100, np.argmax(predict_prob)))



